{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec8f86-9d0e-475a-bbc3-b7c601c08c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import streamlit as st \n",
    "import pandas as pd \n",
    "import dotenv\n",
    "from openai import AzureOpenAI\n",
    "from itertools import chain\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from googleapiclient.discovery import build\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756186b-8309-43b9-875f-a7f3ae52e6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40223d-9878-4efa-8dfb-c1a4d9c7ddcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e962ac4-3cc8-4371-90eb-ccee6625b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotated_text import annotated_text\n",
    "from presidio_analyzer import (\n",
    "    AnalyzerEngine,\n",
    "    RecognizerResult,\n",
    "    RecognizerRegistry,\n",
    "    PatternRecognizer,\n",
    "    Pattern,\n",
    ")\n",
    "from presidio_analyzer.nlp_engine import NlpEngine\n",
    "from presidio_analyzer import RecognizerRegistry\n",
    "from presidio_analyzer.nlp_engine import (\n",
    "    NlpEngine,\n",
    "    NlpEngineProvider,\n",
    ")\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c6b2b-6438-480b-91b3-21b4e2907b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_TYPE=\"Azure\" #or \"openai\"\n",
    "OPENAI_KEY=\"2ab00caaf066494089f9982725837f61\"\n",
    "OPENAI_API_VERSION=\"2023-09-15-preview\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://aai-applications-openai-services.openai.azure.com/\"\n",
    "AZURE_OPENAI_DEPLOYMENT=\"AAI-Applications-Completion\"\n",
    "GOOGLE_CUSTOM_SEARCH_ID=\"7706819d237454897\"\n",
    "GOOGLE_CUSTOM_SEARCH_KEY=\"AIzaSyBA90JEeY0-elLrs5uwXr4-6i1BfLAdMVY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90866c47-a6e2-4dfb-ae17-b42a41571cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OPENAI_ENDPOINT, \n",
    "    api_key = OPENAI_KEY,  \n",
    "    api_version=OPENAI_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b47485-caab-4e49-a08e-8cd1bebb826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "            \"spaCy/en_core_web_lg\",\n",
    "            \"flair/ner-english-large\",\n",
    "            \"HuggingFace/obi/deid_roberta_i2b2\",\n",
    "            \"HuggingFace/StanfordAIMI/stanford-deidentifier-base\",\n",
    "            \"stanza/en\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696eee97-35f2-49dd-93fa-55a86596e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"google_engine_id\"] = GOOGLE_CUSTOM_SEARCH_ID\n",
    "args[\"openai_client\"] = client\n",
    "args[\"custom_search_key\"] = GOOGLE_CUSTOM_SEARCH_KEY\n",
    "args[\"q\"] = \"Alison Goldfrapp\"\n",
    "args[\"k\"] = 50\n",
    "args[\"model\"] = \"spaCy/en_core_web_lg\"\n",
    "args[\"https_flag\"] = True\n",
    "args[\"remove_entities\"] = [\"DATE_TIME\", \"AU_ACN\", \"AU_ABN\", \"AU_MEDICARE\", \"AU_TFN\", \"IN_AADHAAR\", \"IN_VEHICLE_REGISTRATION\", \"IN_PAN\", \"SG_NRIC_FIN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7be3e4-6d23-4ac7-8262-6286e1032d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryExecutor:\n",
    "    \"Creates a QueryExecutor object\"\n",
    "\n",
    "    def __init__(self, args) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a QueryExecutor object\n",
    "        Instance Variables:\n",
    "            query: the query string\n",
    "            k: the number of tuples that we request in the output\n",
    "            google_engine_id: the Google Custom Search Engine ID\n",
    "            client: OpenAI client\n",
    "            engine: the Google Custom Search Engine\n",
    "            seen_urls: the set of URLs that we have already seen\n",
    "            used_queries: the set of queries that we have already used\n",
    "            extractor: the extractor object \n",
    "        \"\"\"\n",
    "\n",
    "        self.q = args[\"q\"]\n",
    "        self.k = args[\"k\"]\n",
    "        self.custom_search_key = args[\"custom_search_key\"]\n",
    "        self.google_engine_id = args[\"google_engine_id\"]\n",
    "        self.client = args[\"openai_client\"]\n",
    "        self.https_flag = args[\"https_flag\"]\n",
    "        self.engine = build(\"customsearch\", \"v1\", developerKey=args[\"custom_search_key\"])\n",
    "        self.seen_urls = set()\n",
    "        self.used_queries = set([self.q])\n",
    "        self.spacy_model_package = args[\"model\"].split(\"/\")[0]\n",
    "        self.spacy_model = \"/\".join(args[\"model\"].split(\"/\")[1:])\n",
    "        self.model = Presidio(self.spacy_model_package, self.spacy_model)\n",
    "        self.extractor = presidioExtractor()\n",
    "\n",
    "    def getQueryResult(self, query: str, k, https_flag: bool) -> List:\n",
    "        \"\"\"\n",
    "        Get the top 10 results for a given query from Google Custom Search API\n",
    "        Source: https://github.com/googleapis/google-api-python-client/blob/main/samples/customsearch/main.py\n",
    "        \"\"\"\n",
    "        if https_flag:\n",
    "            full_res = (\n",
    "                self.engine.cse()\n",
    "                .list(\n",
    "                    q=query,\n",
    "                    cx=self.google_engine_id,\n",
    "                )\n",
    "                .execute()\n",
    "            )\n",
    "        else:\n",
    "            url = f\"https://www.googleapis.com/customsearch/v1?key={self.custom_search_key}&cx={self.google_engine_id}&q={query}\"\n",
    "            full_res = requests.get(url, verify=False).json()\n",
    "         \n",
    "        return full_res[\"items\"][0 : k + 1]\n",
    "\n",
    "\n",
    "    def processText(self, url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get the tokens from a given URL\n",
    "        If webpage retrieval fails (e.g. because of a timeout), it is skipped (None returned)\n",
    "\n",
    "        Extracts the plain text from the URL using Beautiful Soup.\n",
    "        If the resulting plain text is longer than 10,000 characters, it is truncated.\n",
    "        Only the text in the <p> tags is processed.\n",
    "\n",
    "        Parameters:\n",
    "            url (str) - the URL to process\n",
    "        Returns:\n",
    "            List[str] - the list of tokens\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"        Fetching text from url ...\")\n",
    "            page = requests.get(url, timeout=5, verify=self.https_flag)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Error processing {url}: The request timed out. Moving on...\")\n",
    "            return None\n",
    "        try:\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            html_blocks = soup.find_all(\"p\")\n",
    "            text = \"\"\n",
    "            for block in html_blocks:\n",
    "                text += block.get_text()\n",
    "\n",
    "            if text != \"\":\n",
    "                text_len = len(text)\n",
    "                print(\n",
    "                    f\"        Trimming webpage content from {text_len} to 10000 characters\"\n",
    "                )\n",
    "                preprocessed_text = (text[:10000]) if text_len > 10000 else text\n",
    "                print(\n",
    "                    f\"        Webpage length (num characters): {len(preprocessed_text)}\"\n",
    "                )\n",
    "                # Removing redundant newlines and some whitespace characters.\n",
    "                preprocessed_text = re.sub(\"\\t+\", \" \", preprocessed_text)\n",
    "                preprocessed_text = re.sub(\"\\n+\", \" \", preprocessed_text)\n",
    "                preprocessed_text = re.sub(\" +\", \" \", preprocessed_text)\n",
    "                preprocessed_text = preprocessed_text.replace(\"\\u200b\", \"\")\n",
    "\n",
    "                return preprocessed_text\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}. Moving on ...\")\n",
    "            return None\n",
    "\n",
    "    def parseResult(self, result: Dict[str, str]) -> None:\n",
    "        \"\"\"\n",
    "        Parse the result of a query.\n",
    "        Exposed function for use by main function.\n",
    "        Parameters:\n",
    "            result (dict) - one item as returned as the result of a query\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        url = result[\"link\"]\n",
    "        if url not in self.seen_urls:\n",
    "            self.seen_urls.add(url)\n",
    "            text = self.processText(url)\n",
    "            if not text:\n",
    "                return None\n",
    "        return text\n",
    "\n",
    "    def checkContinue(self) -> bool:\n",
    "        \"\"\"\n",
    "        Evaluate if we have evaluated at least k tuples, ie continue or halt.\n",
    "        Parameters: None\n",
    "        Returns: bool (True if we need to find more relations, else False)\n",
    "        \"\"\"\n",
    "        \n",
    "        lst = list(chain.from_iterable(self.extractor.related_entities))\n",
    "        ent_count = len(set(lst))\n",
    "        print(f\"entity count: {ent_count}\")\n",
    "        return ent_count < self.k\n",
    "\n",
    "    def getNewQuery(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Creates a new query.\n",
    "        Select from X a tuple y such that y has not been used for querying yet\n",
    "        Create a query q from tuple y by concatenating\n",
    "        the attribute values together.\n",
    "        If no such y tuple exists, then stop/return None.\n",
    "        (ISE has \"stalled\" before retrieving k high-confidence tuples.)\n",
    "\n",
    "        Parameters:\n",
    "            None\n",
    "        Returns:\n",
    "            query (str) if available; else None\n",
    "        \"\"\"\n",
    "        # Iterating through extracted tuples\n",
    "        for relation in list(self.extractor.related_entities):\n",
    "            tmp_query = \" \".join(relation)\n",
    "            # Checking if query has been used\n",
    "            if tmp_query not in self.used_queries:\n",
    "                # Adding query to used queries\n",
    "                self.used_queries.add(relation)\n",
    "                # Setting new query\n",
    "                self.q = tmp_query\n",
    "                return self.q\n",
    "            # No valid query found\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038d180-462f-4913-b7be-e6ff19df7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Presidio  class\"\n",
    "import json\n",
    "import re\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "import openai\n",
    "import spacy\n",
    "\n",
    "\n",
    "class Presidio:\n",
    "    \"\"\"\n",
    "    Presidio class\n",
    "    \"\"\"\n",
    "    def __init__(self, spacy_model_package, spacy_model):\n",
    "        \"\"\"\n",
    "        Initialize a presidioPredictor object\n",
    "        Parameters:\n",
    "            model: the spaCy model to use\n",
    "        \"\"\"\n",
    "        self.model_package = spacy_model_package\n",
    "        self.model = spacy_model\n",
    "        self.ta_key = \"\"\n",
    "        self.ta_endpoint = \"\"\n",
    "        self.nlp_engine, self.registry = self.nlp_engine_and_registry(self.model)\n",
    "        self.analyzer = AnalyzerEngine(nlp_engine=self.nlp_engine, registry=self.registry)\n",
    "        self.entities = self.analyzer.get_supported_entities()\n",
    "\n",
    "                \n",
    "    def nlp_engine_and_registry(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        ) -> Tuple[NlpEngine, RecognizerRegistry]:\n",
    "        \"\"\"Create the NLP Engine instance based on the requested model.\n",
    "        :param model_family: Which model package to use for NER.\n",
    "        :param model_path: Which model to use for NER. E.g.,\n",
    "            \"StanfordAIMI/stanford-deidentifier-base\",\n",
    "            \"obi/deid_roberta_i2b2\",\n",
    "            \"en_core_web_lg\"\n",
    "        :param ta_key: Key to the Text Analytics endpoint (only if model_path = \"Azure Text Analytics\")\n",
    "        :param ta_endpoint: Endpoint of the Text Analytics instance (only if model_path = \"Azure Text Analytics\")\n",
    "        \"\"\"\n",
    "        nlp_configuration = {\n",
    "            \"nlp_engine_name\": \"spacy\",\n",
    "            \"models\": [{\"lang_code\": \"en\", \"model_name\": model_path}],\n",
    "            \"ner_model_configuration\": {\n",
    "              \"model_to_presidio_entity_mapping\": {\n",
    "                \"PER\": \"PERSON\",\n",
    "                \"PERSON\": \"PERSON\",\n",
    "                \"NORP\": \"NRP\",\n",
    "                \"FAC\": \"FACILITY\",\n",
    "                \"LOC\": \"LOCATION\",\n",
    "                \"GPE\": \"LOCATION\",\n",
    "                \"LOCATION\": \"LOCATION\",\n",
    "                \"ORG\": \"ORGANIZATION\",\n",
    "                \"ORGANIZATION\": \"ORGANIZATION\",\n",
    "                \"DATE\": \"DATE_TIME\",\n",
    "                \"TIME\": \"DATE_TIME\",\n",
    "               },\n",
    "               \"low_confidence_score_multiplier\": 0.4,\n",
    "               \"low_score_entity_names\": [\"ORG\", \"ORGANIZATION\"],\n",
    "             },\n",
    "        }\n",
    "        nlp_engine = NlpEngineProvider(nlp_configuration=nlp_configuration).create_engine()\n",
    "        registry = RecognizerRegistry()\n",
    "        registry.load_predefined_recognizers(nlp_engine=nlp_engine)\n",
    "\n",
    "        return nlp_engine, registry\n",
    "      \n",
    "\n",
    "    def annotate(self, text: str, analyze_results: List[RecognizerResult]):\n",
    "        \"\"\"Highlight the identified PII entities on the original text\n",
    "        :param text: Full text\n",
    "        :param analyze_results: list of results from presidio analyzer engine\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "\n",
    "        # Use the anonymizer to resolve overlaps\n",
    "        results = self.extractor.anonymize(\n",
    "            text=text,\n",
    "            operator=\"highlight\",\n",
    "            analyze_results=analyze_results,\n",
    "        )\n",
    "\n",
    "        # sort by start index\n",
    "        results = sorted(results.items, key=lambda x: x.start)\n",
    "        for i, res in enumerate(results):\n",
    "            if i == 0:\n",
    "                tokens.append(text[: res.start])\n",
    "\n",
    "            # append entity text and entity type\n",
    "            tokens.append((text[res.start : res.end], res.entity_type))\n",
    "\n",
    "            # if another entity coming i.e. we're not at the last results element, add text up to next entity\n",
    "            if i != len(results) - 1:\n",
    "                tokens.append(text[res.end : results[i + 1].start])\n",
    "            # if no more entities coming, add all remaining text\n",
    "            else:\n",
    "                tokens.append(text[res.end :])\n",
    "        return tokens\n",
    "\n",
    "    def create_ad_hoc_deny_list_recognizer(\n",
    "        deny_list=Optional[List[str]],\n",
    "    ) -> Optional[PatternRecognizer]:\n",
    "        if not deny_list:\n",
    "            return None\n",
    "\n",
    "        deny_list_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"GENERIC_PII\", deny_list=deny_list\n",
    "        )\n",
    "        return deny_list_recognizer\n",
    "\n",
    "    def create_ad_hoc_regex_recognizer(\n",
    "        regex: str, entity_type: str, score: float, context: Optional[List[str]] = None\n",
    "    ) -> Optional[PatternRecognizer]:\n",
    "        if not regex:\n",
    "            return None\n",
    "            pattern = Pattern(name=\"Regex pattern\", regex=regex, score=score)\n",
    "            regex_recognizer = PatternRecognizer(\n",
    "                supported_entity=entity_type, patterns=[pattern], context=context\n",
    "            )\n",
    "            return regex_recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83185ee-c2c6-4603-93f6-57a0907c0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Presidio Extractor class\"\n",
    "import json\n",
    "import re\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "import openai\n",
    "import spacy\n",
    "\n",
    "\n",
    "class presidioExtractor:\n",
    "    \"\"\"\n",
    "    Presidio Extractor class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=\"en_core_web_lg\"):\n",
    "        \"\"\"\n",
    "        Initialize a presidioPredictor object\n",
    "        Parameters:\n",
    "            model: the spaCy model to use\n",
    "        \"\"\"\n",
    "        self.nlp = spacy.load(model)\n",
    "        self.related_entities = set()\n",
    "\n",
    "    def get_related_entities(self, text: str, analyze_results: List[RecognizerResult]) ->  pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Exposed function to take in text and return named entities\n",
    "        Parameters:\n",
    "            text: the text to extract entities from\n",
    "        Returns:\n",
    "            entities: a list of tuples of the form (subject, object)\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame.from_records([r.to_dict() for r in analyze_results])\n",
    "        df[\"text\"] = [text[res.start : res.end] for res in analyze_results]\n",
    "\n",
    "        df_subset = df[[\"entity_type\", \"text\", \"start\", \"end\", \"score\"]].rename(\n",
    "                         {\n",
    "                            \"entity_type\": \"entity_type\",\n",
    "                            \"text\": \"text\",\n",
    "                            \"start\": \"start\",\n",
    "                            \"end\": \"end\",\n",
    "                            \"score\": \"confidence\",\n",
    "                        },\n",
    "                        axis=1,\n",
    "                    )\n",
    "        df_subset[\"Text\"] = [text[res.start : res.end] for res in analyze_results]\n",
    "\n",
    "        analysis_explanation_df = pd.DataFrame.from_records(\n",
    "                            [r.analysis_explanation.to_dict() for r in analyze_results]\n",
    "        )\n",
    "        df_subset = pd.concat([df_subset, analysis_explanation_df], axis=1)\n",
    " \n",
    "        self.related_entities.add(tuple(list(df.groupby(['text']).groups))) \n",
    "        \n",
    "        return df_subset\n",
    "\n",
    "    def anonymize(\n",
    "        self,\n",
    "        text: str,\n",
    "        operator: str,\n",
    "        analyze_results: List[RecognizerResult],\n",
    "        mask_char: Optional[str] = None,\n",
    "        number_of_chars: Optional[str] = None,\n",
    "        encrypt_key: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"Anonymize identified input using Presidio Anonymizer.\n",
    "        :param text: Full text\n",
    "        :param operator: Operator name\n",
    "        :param mask_char: Mask char (for mask operator)\n",
    "        :param number_of_chars: Number of characters to mask (for mask operator)\n",
    "        :param encrypt_key: Encryption key (for encrypt operator)\n",
    "        :param analyze_results: list of results from presidio analyzer engine\n",
    "        \"\"\"\n",
    "\n",
    "        if operator == \"mask\":\n",
    "            operator_config = {\n",
    "                \"type\": \"mask\",\n",
    "                \"masking_char\": mask_char,\n",
    "                \"chars_to_mask\": number_of_chars,\n",
    "                \"from_end\": False,\n",
    "            }\n",
    "\n",
    "        # Define operator config\n",
    "        elif operator == \"encrypt\":\n",
    "            operator_config = {\"key\": encrypt_key}\n",
    "        elif operator == \"highlight\":\n",
    "            operator_config = {\"lambda\": lambda x: x}\n",
    "        else:\n",
    "            operator_config = None\n",
    "\n",
    "        # Change operator if needed as intermediate step\n",
    "        if operator == \"highlight\":\n",
    "            operator = \"custom\"\n",
    "        elif operator == \"synthesize\":\n",
    "            operator = \"replace\"\n",
    "        else:\n",
    "            operator = operator\n",
    "\n",
    "        res = AnonymizerEngine().anonymize(\n",
    "            text,\n",
    "            analyze_results,\n",
    "            operators={\"DEFAULT\": OperatorConfig(operator, operator_config)},\n",
    "        )\n",
    "        return res\n",
    "\n",
    "\n",
    "    def annotate(self, text: str, analyze_results: List[RecognizerResult]):\n",
    "        \"\"\"Highlight the identified PII entities on the original text\n",
    "           :param text: Full text\n",
    "           :param analyze_results: list of results from presidio analyzer engine\n",
    "         \"\"\"\n",
    "        tokens = []\n",
    "\n",
    "        # Use the anonymizer to resolve overlaps\n",
    "        results = self.anonymize(\n",
    "            text=text,\n",
    "            operator=\"highlight\",\n",
    "            analyze_results=analyze_results,\n",
    "        )\n",
    "\n",
    "        # sort by start index\n",
    "        results = sorted(results.items, key=lambda x: x.start)\n",
    "        for i, res in enumerate(results):\n",
    "            if i == 0:\n",
    "               tokens.append(text[: res.start])\n",
    "\n",
    "            # append entity text and entity type\n",
    "            tokens.append((text[res.start : res.end], res.entity_type))\n",
    "\n",
    "            # if another entity coming i.e. we're not at the last results element, add text up to next entity\n",
    "            if i != len(results) - 1:\n",
    "                tokens.append(text[res.end : results[i + 1].start])\n",
    "            # if no more entities coming, add all remaining text\n",
    "            else:\n",
    "                tokens.append(text[res.end :])\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271b283-e41e-4d48-acb7-37a8afb90764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "executor = QueryExecutor(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4b251-ab8c-4c07-a4b8-4be708742eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterate_further = True\n",
    "iterations = 0\n",
    "\n",
    "res = []\n",
    "txt = []\n",
    "\n",
    "while iterate_further:\n",
    "        # Get the top 10 results for the current query\n",
    "        results = executor.getQueryResult(executor.q, 10, executor.https_flag)\n",
    "        print(f\"=========== Iteration: {iterations} - Query: {executor.q} ===========\")\n",
    "        for i, item in enumerate(results):\n",
    "            print(f\"URL ( {i+1} / 50): {item['link']}\")\n",
    "            text =  executor.processText(item['link'])\n",
    "            if text:\n",
    "                txt.append(text)\n",
    "                analyze_results = executor.model.analyzer.analyze(\n",
    "                    text=text, \n",
    "                    entities=executor.model.entities,\n",
    "                    language=\"en\",\n",
    "                    return_decision_process=True\n",
    "                )\n",
    "                df = executor.extractor.get_related_entities(text=text,\n",
    "                                                             analyze_results=analyze_results)\n",
    "                res.append(df)\n",
    "            if not executor.checkContinue():\n",
    "                iterate_further = False\n",
    "                break\n",
    "            iterations += 1\n",
    "        # If a new iteration is needed, get the new query\n",
    "        if not executor.getNewQuery():\n",
    "            print(\"No new queries to try\")\n",
    "            print(\"Exiting ...\")\n",
    "            break\n",
    "\n",
    "print(f\"Total # of iterations = {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce64116-355b-41d4-86e0-2465fa1897dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf = pd.concat(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c3bfe-dd56-4f5f-ae00-7bafb5047c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text = ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1507fa-2e64-466c-b23d-43551262b468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f70dd-46fd-496a-86d3-10051c2040f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotated_tokens = executor.presidio.extractor.annotate(text=text, analyze_results=analyze_results)\n",
    "#annotated_text(*annotated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b805c1c-0d1e-4879-8412-fa4d8b34ea5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7515f-8fa6-46f8-a1f1-393781c20634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entity_names(x):\n",
    "    x['clean_text'] = x['text'].split('.')[0]\n",
    "    x['clean_text'] = x['clean_text'].lower()    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae6a32-9399-4c93-9ee3-ad01a4359c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean df\n",
    "resdf = resdf.apply(clean_entity_names,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cd902-1d76-4c17-9153-cf16187567fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9d391-589a-44ac-9151-e8a6b3c682e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ent_list = []\n",
    "for e in list(resdf[\"clean_text\"]):\n",
    "    ent_list.append(e.split('.')[0])\n",
    "ent_set = set(ent_list)\n",
    "un_ent_lst = list(ent_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64071cfd-d4d6-4a8a-aeb6-c82f57e5df1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "un_ent_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc429a-ca00-47dd-8774-494ef2d3e97c",
   "metadata": {},
   "source": [
    "### Entity Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd5628-6bb9-40eb-adf0-c144efb3d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_graph_v1(ent_list: list, input: str):\n",
    "\n",
    "    SYS_PROMPT = (\"You are a network graph maker who extracts terms and their relations from a given context. \"\n",
    "        \"You are provided with a list of entities and some text.\"\n",
    "        \"Your task is to extract the ontology of terms mentioned in the given context relating to the entities in the list.\"\n",
    "        \"Thought 1: While traversing through each sentence, Think about the key terms mentioned in it.\\n\"\n",
    "            \"\\tTerms may include person (agent), location, organization, date, duration, \\n\"\n",
    "            \"\\tcondition, concept, object, entity  etc.\\n\"\n",
    "            \"\\tTerms should be as atomistic as possible\\n\"\n",
    "        \"Thought 2: Think about how these terms can have one on one relation with other terms.\\n\"\n",
    "            \"\\tTerms that are mentioned in the same sentence or the same paragraph are typically related to each other.\\n\"\n",
    "            \"\\tTerms can be related to many other terms\\n\"\n",
    "        \"Thought 3: Find out the relation between each such related pair of terms. \\n\"\n",
    "        \"Return this list as valid JSON like the following: \\n\"\n",
    "        \"{ \\n\"\n",
    "        \"  graph:[ \\n\"\n",
    "        \"   {\\n\"\n",
    "        '       \"node_1\": \"A concept from extracted ontology\",\\n'\n",
    "        '       \"node_2\": \"A related concept from extracted ontology\",\\n'\n",
    "        '       \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\"\\n' \n",
    "        \"   }, {...}\\n\"\n",
    "        \" ] \\n\"\n",
    "        \"}\"\n",
    "    )\n",
    "\n",
    "    USER_PROMPT = f\"entity_list: ```{ent_list}```, context: ```{input}``` \\n\\n output: \"\n",
    "\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": f\"{SYS_PROMPT}\"},\n",
    "      {\"role\": \"user\", \"content\": f\"{USER_PROMPT}\"}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(model=\"AAI-Applications\", \n",
    "                                              response_format={ \"type\": \"json_object\" },\n",
    "                                              messages=messages)\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab0b23-da4c-4b8c-8a98-ef70a1cc5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "\n",
    "def create_graph(gdf: pd.DataFrame):\n",
    "    G = nx.Graph()\n",
    "    ## Add edges to the graph\n",
    "    for index, row in gdf.iterrows():\n",
    "        G.add_edge(\n",
    "            str(row[\"node_1\"].lower()),\n",
    "            str(row[\"node_2\"].lower()),\n",
    "            title=row[\"edge\"],\n",
    "        )\n",
    "\n",
    "    return G    \n",
    "\n",
    "\n",
    "def create_vis(g):\n",
    "    net = Network(\n",
    "        notebook=True,\n",
    "        bgcolor=\"#FFFFF\",\n",
    "        cdn_resources=\"remote\",\n",
    "        height=\"800px\",\n",
    "        width=\"100%\",\n",
    "        select_menu=True,\n",
    "        font_color=\"#cccccc\",\n",
    "        filter_menu=False,\n",
    "    )\n",
    "\n",
    "    net.from_nx(g)\n",
    "    # net.repulsion(node_distance=150, spring_length=400)\n",
    "    #net.force_atlas_2based(central_gravity=0.015, gravity=-31)\n",
    "    net.barnes_hut(gravity=-18100, central_gravity=5.05, spring_length=380)\n",
    "\n",
    "    # net.show(graph_output_directory)\n",
    "    net.show_buttons(filter_=['physics'])\n",
    "    net.show(\"knowledge_graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f97df-6215-4f73-ae5b-8015b347cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = knowledge_graph_v1(un_ent_lst, plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110d436-19e7-4138-948a-297c31898505",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = json.loads(kg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8705e1-4d76-4960-9e0a-4f7b5bfe0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = pd.DataFrame(j['graph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84aa6f-fe6e-4d77-93cb-4e93526ccd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = create_graph(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef99f7-0cb1-4e84-bbf9-bb67a9c1b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(\n",
    "    notebook=True,\n",
    "    bgcolor=\"#FFFFF\",\n",
    "    cdn_resources=\"remote\",\n",
    "    height=\"900px\",\n",
    "    width=\"100%\",\n",
    "    select_menu=True,\n",
    "    font_color=\"#6a6b6f\",\n",
    "    filter_menu=True,\n",
    ")\n",
    "\n",
    "net.from_nx(g)\n",
    "# net.repulsion(node_distance=150, spring_length=400)\n",
    "#net.force_atlas_2based(central_gravity=0.015, gravity=-31)\n",
    "net.barnes_hut(gravity=-18100, central_gravity=5.05, spring_length=380)\n",
    "\n",
    "# net.show(graph_output_directory)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.show(\"knowledge_graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d2726-d1c2-4628-9497-80dbc6801886",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(g, 'soi-entity-graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea908167-9b24-4db6-982b-5137c8c2cac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entity-analytics",
   "language": "python",
   "name": "entity-analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
